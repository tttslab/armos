Scripts for acoustic-to-articulatory inversion by analysis-by-synthesis strategy.
Estimater is optimized utilizing policy gradient algorithm.
The actor outputs gaussian distributions and articulatory movements are estimated with Maximum Likelihood Parameter Generation (MLGP).
The articulatory synthesizer is VocalTractLab 2.1.
http://www.vocaltractlab.de/

from VocalTractLab 2.1
-JD2.speaker: Adult male speaker file.
-TractSeqToWave_Example.py: Modified a little for training.
-TractSeqToWave_synthesis.py: Modified a little for evaluation. 
-VocalTractLabApi.so: VTL library

exp: Trained model is saved in this directory.
     "p" denotes actor (policy) and "q" denotes critic (Q func).
log: Log file is saved
datalist
-training_list.txt: List of training data path. Relative path from feat_dir.
-testing_list.txt: List of test data path. Actually do not use.
-validation_list.txt: List of validation data path. Actually do not use.

testdata
-testdata.npy: Correct tract parameters
-num.wav: Test sound

config.ini
-in_size: Dimension of acoustic features
-out_size: The number of articulatory parameters
-batch_size: Batch size for training
-num_sample: Do not use. If you set the output of actor to a distribution, use this parameter.
-num_paral: Parallelize synthesis step.
	    Set a divisor of batch_size.(because of my(shibata) laziness)
-sampling_rate: Sampling rate of original sounds.
-frameRate_Hz: Frame rate of articulatory.
	       For now fixed to 100 in order to correspond to acoustic feature.
-audio_segment: The number of acoustic feature's frames for training.
		This number's frames are randomly extracted from whole utterance, and use for training.
-wave_dir: Directory of original sounds data.
-feat_dir: Extracted acoustic features are saved in this directory.
-feat_type: You can select MFCC, FBANK or log-FBANK.
-hidden_size: The number of units of LSTM.
-num_layers: The number of stacked BLSTM.
-delta: Necessary for Parameter space noise. Refer to the original paper.
-sigma_init: Initial value of gaussian distribution's standard deviation.
-adjust_step: Adjust sigma per this value.
-learning_rate: Learning rate for actor (SGD) and critic (Adam).

eval.py: Calculate RMSE of articulatory using test data.
	 The result is saved in log/eval.csv.
	 Each column corresponds to each tract parameters. About the order, refer to a speaker file.
	 Note that 9th parameter (WC) is meaningless.
eval.sh: Evaluation script per 1000 iterations.
models.py: Define actor and critic.
pretrain.py: Make the output of actor close to 0.5.
	     Log file is saved in log/pretrain.log.
	     Trained model is saved in exp/pretrain.model.
train.py: Training script. The log is saved in log/train.log.
	  I recommend below command to run this:
	    	      nohup python train.py 1>/dev/null 2>log/err.log
validation.py: For varidation. The log is saved in log/valid.log.
               Usage:
                python validation.py iteration validation batch_size num_cpu
                ex) python validation.py 10000 validation 100 10 1>/dev/null
test.py: For test. The log is saved in log/test.log.
                python test.py iteration testing batch_size num_cpu
                ex) python test.py 10000 testing 100 10 1>/dev/null
utils.py: Some functions are defined.
generate_sound.py: Estimate and synthesize a sound.
		   The head of synthesized sound has extra 50ms sound and its tail has extra 50+alpha ms sound.
		   The output is synthesized.wav.
		   Example command is below:
		      python generate_sound.py path/to/hoge.wav exp/p1000.model
		      sox synthesized.wav output.wav trim 0.05 1.6
requirements.txt: Requirements modules.